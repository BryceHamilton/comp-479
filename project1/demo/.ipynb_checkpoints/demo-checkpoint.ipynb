{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e878aab8",
   "metadata": {},
   "source": [
    "## TODO\n",
    "#### - Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4eb01b-d327-4366-8005-86489ca8e1bd",
   "metadata": {},
   "source": [
    "# Demo File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024b88a-c9a7-427a-8392-cdefa2a9a474",
   "metadata": {},
   "source": [
    "#### + Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458e790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** IMPORTS\n",
    "\n",
    "# stop words from nltk for demo purposes\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# create set of english stop words from nltk\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# import tests\n",
    "import asserts\n",
    "\n",
    "# import pipeline\n",
    "\n",
    "# step 1\n",
    "from pipeline import read_smg\n",
    "\n",
    "# step 1.1\n",
    "from pipeline import segment_documents\n",
    "\n",
    "# step 1.2\n",
    "from pipeline import extract_html_symbols\n",
    "\n",
    "# step 1.3\n",
    "from pipeline import extract_punctuation\n",
    "from pipeline import extract_punctuation_keep_digits\n",
    "\n",
    "# step 2\n",
    "from pipeline import tokenize_doc_str\n",
    "\n",
    "# step 3\n",
    "from pipeline import case_fold_tokens\n",
    "\n",
    "# step 4\n",
    "from pipeline import stem_tokens\n",
    "\n",
    "# step 5\n",
    "from pipeline import filter_out_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527942e3",
   "metadata": {},
   "source": [
    "### STEP 1: Read Files --> Output Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb82ac0-1ac8-4528-bd49-ce5b8376897e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file num:  00\n",
      "file content, first 1000 chars:  <!DOCTYPE lewis SYSTEM \"lewis.dtd\">\n",
      "<REUTERS TOPICS=\"YES\" LEWISSPLIT=\"TRAIN\" CGISPLIT=\"TRAINING-SET\" OLDID=\"5544\" NEWID=\"1\">\n",
      "<DATE>26-FEB-1987 15:01:01.79</DATE>\n",
      "<TOPICS><D>cocoa</D></TOPICS>\n",
      "<PLACES><D>el-salvador</D><D>usa</D><D>uruguay</D></PLACES>\n",
      "<PEOPLE></PEOPLE>\n",
      "<ORGS></ORGS>\n",
      "<EXCHANGES></EXCHANGES>\n",
      "<COMPANIES></COMPANIES>\n",
      "<UNKNOWN> \n",
      "&#5;&#5;&#5;C T\n",
      "&#22;&#22;&#1;f0704&#31;reute\n",
      "u f BC-BAHIA-COCOA-REVIEW   02-26 0105</UNKNOWN>\n",
      "<TEXT>&#2;\n",
      "<TITLE>BAHIA COCOA REVIEW</TITLE>\n",
      "<DATELINE>    SALVADOR, Feb 26 - </DATELINE><BODY>Showers continued throughout the week in\n",
      "the Bahia cocoa zone, alleviating the drought since early\n",
      "January and improving prospects for the coming temporao,\n",
      "although normal humidity levels have not been restored,\n",
      "Comissaria Smith said in its weekly review.\n",
      "    The dry period means the temporao will be late this year.\n",
      "    Arrivals for the week ended February 22 were 155,221 bags\n",
      "of 60 kilos making a cumulative total for the season of 5.93\n",
      "mln against 5.81 at the sa\n"
     ]
    }
   ],
   "source": [
    "# set file path\n",
    "file_paths  = ['reuters21578/reut2-000.sgm']\n",
    "\n",
    "# return generator from reader\n",
    "read_gen = read_smg(file_paths)\n",
    "\n",
    "# extract first file\n",
    "file_num, file_content = next(read_gen)\n",
    "\n",
    "# run tests\n",
    "asserts.reader_tests(file_num, file_content)\n",
    "\n",
    "# sample output\n",
    "print('file num: ', file_num)\n",
    "print('file content, first 1000 chars: ', file_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbca7a7-87df-42d2-b8af-de8782a02bed",
   "metadata": {},
   "source": [
    "### STEP 1.1: Input Files --> Output Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c55edd-abce-4b33-a42a-6a259ea002a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id:  1\n",
      "doc_str, first 1000 chars:  Showers continued throughout the week in\n",
      "the Bahia cocoa zone, alleviating the drought since early\n",
      "January and improving prospects for the coming temporao,\n",
      "although normal humidity levels have not been restored,\n",
      "Comissaria Smith said in its weekly review.\n",
      "    The dry period means the temporao will be late this year.\n",
      "    Arrivals for the week ended February 22 were 155,221 bags\n",
      "of 60 kilos making a cumulative total for the season of 5.93\n",
      "mln against 5.81 at the same stage last year. Again it seems\n",
      "that cocoa delivered earlier on consignment was included in the\n",
      "arrivals figures.\n",
      "    Comissaria Smith said there is still some doubt as to how\n",
      "much old crop cocoa is still available as harvesting has\n",
      "practically come to an end. With total Bahia crop estimates\n",
      "around 6.4 mln bags and sales standing at almost 6.2 mln there\n",
      "are a few hundred thousand bags still in the hands of farmers,\n",
      "middlemen, exporters and processors.\n",
      "    There are doubts as to how much of this cocoa would be fit\n",
      "for export \n"
     ]
    }
   ],
   "source": [
    "# return generator from doc segmenter\n",
    "doc_gen = segment_documents(file_content)\n",
    "\n",
    "# extract first doc\n",
    "doc_id, doc_str = next(doc_gen)\n",
    "\n",
    "# run tests\n",
    "asserts.segmenter_test(doc_id, doc_str)\n",
    "\n",
    "# sample output\n",
    "print('doc_id: ', doc_id)\n",
    "print('doc_str, first 1000 chars: ', doc_str[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91168d4-44e2-4c1a-9f98-854e5ae1e040",
   "metadata": {},
   "source": [
    "### STEP 1.2, 1.3: Input Document --> Output Document (without HTML Symbols, Punctuation, Numbers, or Linebreaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b6e8e84-ffcb-459c-90e1-96d168999853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_str, last 1000 chars:  o 4,450 dlrs and at\n",
      "2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\n",
      "2.27 times New York Dec, Comissaria Smith said.\n",
      "    Destinations were the U.S., Covertible currency areas,\n",
      "Uruguay and open ports.\n",
      "    Cake sales were registered at 785 to 995 dlrs for\n",
      "March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\n",
      "New York Dec for Oct/Dec.\n",
      "    Buyers were the U.S., Argentina, Uruguay and convertible\n",
      "currency areas.\n",
      "    Liquor sales were limited with March/April selling at 2,325\n",
      "and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\n",
      "York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\n",
      "Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\n",
      "said.\n",
      "    Total Bahia sales are currently estimated at 6.13 mln bags\n",
      "against the 1986/87 crop and 1.06 mln bags against the 1987/88\n",
      "crop.\n",
      "    Final figures for the period to February 28 are expected to\n",
      "be published by the Brazilian Cocoa Trade Commission after\n",
      "carnival which ends midday on February 27.\n",
      " Reuter\n",
      "&#3;\n"
     ]
    }
   ],
   "source": [
    "print('doc_str, last 1000 chars: ', doc_str[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dfec2d-3562-4764-bd17-9ab06b049cfd",
   "metadata": {},
   "source": [
    "as seen above, html symbol is present at the end, as well as line breaks, punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294b1d22-4aec-4997-a321-b642bbc4bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC STR NO PUNC, LAST 1000 CHARS:  t 4400 and 4415 dlrs Aug Sept at 4351 to 4450 dlrs and at 227 and 228 times New York Sept and Oct Dec at 4480 dlrs and 227 times New York Dec Comissaria Smith said     Destinations were the US Covertible currency areas Uruguay and open ports     Cake sales were registered at 785 to 995 dlrs for March April 785 dlrs for May 753 dlrs for Aug and 039 times New York Dec for Oct Dec     Buyers were the US Argentina Uruguay and convertible currency areas     Liquor sales were limited with March April selling at 2325 and 2380 dlrs June July at 2375 dlrs and at 125 times New York July Aug Sept at 2400 dlrs and at 125 times New York Sept and Oct Dec at 125 times New York Dec Comissaria Smith said     Total Bahia sales are currently estimated at 613 mln bags against the 1986 87 crop and 106 mln bags against the 1987 88 crop     Final figures for the period to February 28 are expected to be published by the Brazilian Cocoa Trade Commission after carnival which ends midday on February 27  Reuter  \n",
      "\n",
      "\n",
      "DOC STR NO PUNC NO DIGIT, LAST 1000 CHARS:  arch April sold at   and  dlrs     April May butter went at  times New York May June July at  and  dlrs Aug Sept at  to  dlrs and at  and  times New York Sept and Oct Dec at  dlrs and  times New York Dec Comissaria Smith said     Destinations were the US Covertible currency areas Uruguay and open ports     Cake sales were registered at  to  dlrs for March April  dlrs for May  dlrs for Aug and  times New York Dec for Oct Dec     Buyers were the US Argentina Uruguay and convertible currency areas     Liquor sales were limited with March April selling at  and  dlrs June July at  dlrs and at  times New York July Aug Sept at  dlrs and at  times New York Sept and Oct Dec at  times New York Dec Comissaria Smith said     Total Bahia sales are currently estimated at  mln bags against the   crop and  mln bags against the   crop     Final figures for the period to February  are expected to be published by the Brazilian Cocoa Trade Commission after carnival which ends midday on February   Reuter  \n"
     ]
    }
   ],
   "source": [
    "# remove html symbols\n",
    "doc_str_no_html = extract_html_symbols(doc_str)\n",
    "\n",
    "# option to keep digits\n",
    "doc_str_no_punc = extract_punctuation_keep_digits(doc_str_no_html)\n",
    "\n",
    "# option to remove digits\n",
    "doc_str_no_punc_no_dig = extract_punctuation(doc_str_no_html)\n",
    "\n",
    "print('DOC STR NO PUNC, LAST 1000 CHARS: ', doc_str_no_punc[-1000:])\n",
    "print('\\n')\n",
    "print('DOC STR NO PUNC NO DIGIT, LAST 1000 CHARS: ', doc_str_no_punc_no_dig[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f35951-ca66-4f58-93dd-dccf6f9d1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tests\n",
    "asserts.extractor_test(doc_str_no_punc_no_dig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea9787",
   "metadata": {},
   "source": [
    "### STEP 2: Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a71bfff-df74-4ff8-88f4-9e7927162705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 'Showers')\n",
      "('1', 'continued')\n",
      "('1', 'throughout')\n",
      "('1', 'the')\n",
      "('1', 'week')\n",
      "('1', 'in')\n",
      "('1', 'the')\n",
      "('1', 'Bahia')\n",
      "('1', 'cocoa')\n",
      "('1', 'zone')\n",
      "('1', 'alleviating')\n",
      "('1', 'the')\n",
      "('1', 'drought')\n",
      "('1', 'since')\n",
      "('1', 'early')\n"
     ]
    }
   ],
   "source": [
    "doc_str = doc_str_no_punc_no_dig\n",
    "\n",
    "# tokenize document\n",
    "token_tuples = tokenize_doc_str(doc_id, doc_str)\n",
    "\n",
    "# print sample of tokens\n",
    "for token_tuple in token_tuples[:15]:\n",
    "    print(token_tuple)\n",
    "    \n",
    "# run tests\n",
    "asserts.tokenizer_test(token_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740099d",
   "metadata": {},
   "source": [
    "### STEP 3: Case Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1778a806-b52f-46d0-aa55-a8c07449ecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 'showers')\n",
      "('1', 'continued')\n",
      "('1', 'throughout')\n",
      "('1', 'the')\n",
      "('1', 'week')\n",
      "('1', 'in')\n",
      "('1', 'the')\n",
      "('1', 'bahia')\n",
      "('1', 'cocoa')\n",
      "('1', 'zone')\n",
      "('1', 'alleviating')\n",
      "('1', 'the')\n",
      "('1', 'drought')\n",
      "('1', 'since')\n",
      "('1', 'early')\n"
     ]
    }
   ],
   "source": [
    "# case-fold tokens\n",
    "token_tuples = case_fold_tokens(token_tuples)\n",
    "\n",
    "# print sample of case-folded tokens\n",
    "for token_tuple in token_tuples[:15]:\n",
    "    print(token_tuple)\n",
    "    \n",
    "# run tests\n",
    "asserts.case_folder_test(token_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093cff46",
   "metadata": {},
   "source": [
    "### STEP 4: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d571d1c0-9610-49c4-ab40-9b1cc6de029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 'shower')\n",
      "('1', 'continu')\n",
      "('1', 'throughout')\n",
      "('1', 'the')\n",
      "('1', 'week')\n",
      "('1', 'in')\n",
      "('1', 'the')\n",
      "('1', 'bahia')\n",
      "('1', 'cocoa')\n",
      "('1', 'zone')\n",
      "('1', 'allevi')\n",
      "('1', 'the')\n",
      "('1', 'drought')\n",
      "('1', 'sinc')\n",
      "('1', 'earli')\n"
     ]
    }
   ],
   "source": [
    "# stem tokens\n",
    "token_tuples = stem_tokens(token_tuples)\n",
    "\n",
    "# print sample of case-folded tokens\n",
    "for token_tuple in token_tuples[:15]:\n",
    "    print(token_tuple)\n",
    "    \n",
    "# run tests\n",
    "asserts.stemmer_test(token_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db3191",
   "metadata": {},
   "source": [
    "### STEP 5: Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee215445-74b8-42ec-9429-3b7af11c0833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 'shower')\n",
      "('1', 'continu')\n",
      "('1', 'throughout')\n",
      "('1', 'week')\n",
      "('1', 'bahia')\n",
      "('1', 'cocoa')\n",
      "('1', 'zone')\n",
      "('1', 'allevi')\n",
      "('1', 'drought')\n",
      "('1', 'sinc')\n",
      "('1', 'earli')\n",
      "('1', 'januari')\n",
      "('1', 'improv')\n",
      "('1', 'prospect')\n",
      "('1', 'come')\n"
     ]
    }
   ],
   "source": [
    "# filter stop words from tokens\n",
    "token_tuples = filter_out_stop_words(token_tuples, nltk_stop_words)\n",
    "\n",
    "# print sample of stop word filtered tokens\n",
    "for token_tuple in token_tuples[:15]:\n",
    "    print(token_tuple)\n",
    "\n",
    "asserts.stop_word_test(token_tuples, nltk_stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp-479",
   "language": "python",
   "name": "comp-479"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
