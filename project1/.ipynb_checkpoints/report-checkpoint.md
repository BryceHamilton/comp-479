# Report
## Project 1 - COMP 479
#### Bryce Hamilton 40050171
#### Due: October 8, 2021

### Files

#### `output` directory
Output files for each of the 5 steps of pipeline for the first 5 documents present in the first reuters file, generated by running `python run_pipeline.py`

#### `reuters21578` directory
Input Reuters files  
Reuterâ€™s Corpus Reuters-21578  
Source: http://www.daviddlewis.com/resources/testcollections/reuters21578/

#### `helpers` directory
Contains helpers for writing to file, either a string or list

#### `demo` directory
Contains demo file notebook as well as pdf demonstracting each of the steps of the pipeline and tests

#### `pipeline.py`
Core module containing the 5 core steps of the pipeline, along with extra steps and a `run()` function to run the pipeline

#### `run_pipeline.py`
As discussed above, used to run the pipeline and generate output files  
Run pipeline with default arguments: `python run_pipeline.py`  
Run pipeline with file limit (3) and doc limit (4): `python run_pipeline.py 3 4`  
This will write output files for the first 4 documents in the first 3 reuters files

#### `asserts.py`
File to test each step in the pipeline

#### `report.pdf`
File containing report of project


### Pipeline

#### STEP 1 - Reader
Generator of smg file contents.
Given a list of paths to reuters smg files, 
reads in each and yields the file number and document contents.
    
    Parameters:
        file_paths (List[str]): list of file path strings to Reuters smg files
    
    Yields:
        file_num (str): The number of the file that was read, from 0 - 21
        file_content (str): The entire contents of the file

#### STEP 1.1 - Segmentor 
Generator that segments the documents from an sgm_file string.
    
    Parameters:
        sgm_file (str): Content of sgm file
    
    Yields:
        doc_id (str): Id of latest the document parsed from the file
        doc_text (str): Contents of latest the document parsed from the file
    
    
*Note*:  
does not pull out metadata other than doc id
which would be better for extended the pipeline for indexing or other analysis  

*Strengths*:   
      - Includes doc id which will help with indexing,  
      - function is a generator which facilitates pipeline

#### STEP 1.2 - HTML Extractor
Removes html symbols from document string using regex.
    
    Parameters:
        document_str (str): Content of document
    
    Returns:
        document_str (str): The content of the same document with html symbols removed


#### STEP 1.3 - Punctuation Extractor
Removes non-alphabetic characters   
and replaces linebreaks with white space from document string using regex.
    
    Parameters:
        document_str (str): Content of document
    
    Returns:
        document_str (str): The content of the same document without 
                            non-alphabetic characters or linebreaks 
    
*Note*: When digits are kept, decimals are lost, such as 6.28 is converted to 628

#### STEP 2 - Tokenizer
Tokenizes document string using nltk, 
includes document id with token.
    
    Parameters:
        doc_id (str): The id of the document passed
        document_str (str): Content of document
    
    Returns:
        (List[(str, str)]): List of tuples, each with doc id and token

#### STEP 3 - Case Folder
  Case folds list of token tuples.
  Preserves document id with token.
  
    Parameters:
        token_tuples (List[(str, str)]): List of tuples, each with doc id and token
    
    Returns:
        (List[(str, str)]): List of tuples, each with doc id and lower case token

#### STEP 4 - Stemmer
   Stems list of token tuples with nltk Porter Stemmer.
   Preserves document id with token.
   
   
     Parameters:
        token_tuples (List[(str, str)]): List of tuples, each with doc id and token
    
     Returns:
        (List[(str, str)]): List of tuples, each with doc id and stemmed token

#### STEP 5 - Stop Words Filtering

  Removes stop words from token tuples.
  Preserves document id with token.

    Parameters:
        token_tuples (List[(str, str)]): List of tuples, each with doc id and token
    
    Returns:
        (List[(str, str)]): List of tuples, each with doc id and token
      
 *Note*: Common word 'reuters' is kept by default

#### RUN

Runs pipeline.

    Parameters:
        file_names (List[str]): List of file path strings to Reuters sgm files
        doc_limit_per_file (int): Limit to number of documents that should be processed per file
        stop_words (List[str]): List of stop words strings to be used to filter final token list
    
    Returns:
        None
            
            
#### run_pipeline.py

*Note*: running pipeline doesn't faciliate file offset, ie range, simply 0 to limit, similar for doc numbers, no offset
